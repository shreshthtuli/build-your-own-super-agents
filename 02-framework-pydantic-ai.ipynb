{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# 02. Moving to a Framework: PydanticAI\n",
    "\n",
    "In tutorial 01 we wired up a bare-bones agent manually. This lesson upgrades the workflow by adopting **[PydanticAI](https://ai.pydantic.dev/)**, a lightweight agent framework that keeps prompts, tools, and validation in one place. We'll keep the examples compact so you can focus on the core ideas, while also mapping each topic back to the official documentation so you can dig deeper on your own.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-goals",
   "metadata": {},
   "source": [
    "## What you'll learn\n",
    "- How the core `Agent` API maps to models, prompts, and run results.\n",
    "- How to register tools, inject dependencies, and read the `RunContext`.\n",
    "- How to add structured outputs, guardrails (validation + redaction), and retries.\n",
    "- How to stream responses and log the conversation with Logfire.\n",
    "- How to glue everything together in a realistic concierge use-case.\n",
    "- Where to find each concept inside the [PydanticAI docs](https://ai.pydantic.dev/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation roadmap\n",
    "PydanticAI's site is organized by concept. As you experiment in this notebook, keep these reference pages open:\n",
    "\n",
    "| Topic | Docs quick link | Why it matters |\n",
    "| --- | --- | --- |\n",
    "| Agents & models | [`/agents/`](https://ai.pydantic.dev/agents/) | Explains the `Agent` constructor, run methods, and model adapters. |\n",
    "| Tools & dependencies | [`/tools/`](https://ai.pydantic.dev/tools/) | Shows the `@agent.tool` decorator, dependency injection, and context helpers. |\n",
    "| Results & streaming | [`/results/`](https://ai.pydantic.dev/results/) | Documents `RunResult`, streaming iterators, and metadata on each run. |\n",
    "| Guardrails | [`/guardrails/`](https://ai.pydantic.dev/guardrails/) | Covers structured outputs, validation retries, and redaction helpers. |\n",
    "| Observability | [`/logfire/`](https://ai.pydantic.dev/logfire/) | Walks through Logfire setup, spans, and structured traces. |\n",
    "\n",
    "We'll point back to these sections throughout the tutorial so you can connect code to the syntax explained in the docs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 0. Requirements\n",
    "1. Install dependencies locally (already present in this course environment). If you're on your own machine run:\n",
    "   ```bash\n",
    "   pip install pydantic-ai logfire ipykernel\n",
    "   ```\n",
    "2. Provide an API key for the provider you want to use (OpenAI works great). In a terminal run `export OPENAI_API_KEY=...` before launching Jupyter.\n",
    "3. Optional but recommended: create a free [Logfire](https://logfire.pydantic.dev/) account for rich traces.\n",
    "4. Skim the [quickstart](https://ai.pydantic.dev/quickstart/) so the notebook examples feel familiar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environment-check",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"\u26a0\ufe0f  Set the OPENAI_API_KEY environment variable before talking to a hosted model.\")\n",
    "else:\n",
    "    print(\"API key detected \u2705\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Meet `Agent`\n",
    "PydanticAI wraps model calls inside an `Agent`. You configure the model, give it a concise system prompt, and call `run_sync` to execute a turn.\n",
    "\n",
    "We'll use a concierge persona throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anatomy of `Agent(...)`\n",
    "The constructor mirrors the [agent configuration docs](https://ai.pydantic.dev/agents/#agent-constructor):\n",
    "\n",
    "- `model`: A model adapter such as `OpenAIModel`, `AnthropicModel`, or `LiteLLMModel`. Each adapter exposes provider-specific settings via keyword arguments.\n",
    "- `system_prompt`: A concise instruction that frames every conversation turn.\n",
    "- `deps_type`: (Optional) A Pydantic model describing the structured dependencies your tools need.\n",
    "- `result_type`: (Optional) A Pydantic model that constrains the final answer.\n",
    "- `tools`: (Optional) A list of callables registered ahead of time. You can also decorate functions with `@agent.tool` as shown later.\n",
    "\n",
    "Every `Agent` offers synchronous (`run_sync`) and asynchronous (`run`) helpers that return a [`RunResult`](https://ai.pydantic.dev/results/#runresult). We'll use the sync variants to keep the tutorial focused.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "\n",
    "concierge = Agent(\n",
    "    model=OpenAIModel(model=\"gpt-4o-mini\"),\n",
    "    system_prompt=\"You are a friendly city concierge. Keep replies short, positive, and specific to Berlin.\",\n",
    ")\n",
    "\n",
    "intro = concierge.run_sync(\"Welcome a visitor to Berlin in one sentence.\")\n",
    "print(intro.response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-explain",
   "metadata": {},
   "source": [
    "`Agent` accepts the model backend (OpenAI, Anthropic, etc.), a system prompt, and optional extras. `run_sync` sends the user message, waits for the model, and returns a `RunResult` with metadata like `response_text`, tool traces, and tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunResult` carries the `response_text`, a structured `data` attribute when `result_type` is used, token usage, tool call history, and helper methods like `.messages` to inspect the full transcript. Inspect it in the notebook or read the [results guide](https://ai.pydantic.dev/results/) for the complete schema.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Tools + Dependencies: Local Concierge Use-Case\n",
    "Agents become useful once they can call real data. We'll model a concierge that knows Berlin restaurants and events stored in plain Python dictionaries.\n",
    "\n",
    "PydanticAI lets you declare a dependency schema (`deps_type`) and register tool functions with the `@agent.tool` decorator. Tools receive a `RunContext` that exposes the dependencies plus request metadata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why dependencies matter\n",
    "The docs describe dependencies as a way to [share stateful resources](https://ai.pydantic.dev/tools/#dependencies) (database clients, configuration, caches) without global variables. Defining `deps_type` with Pydantic gives you:\n",
    "\n",
    "1. **Validation** \u2013 Data loaded into the dependency object is checked once, so tools always see a consistent shape.\n",
    "2. **Auto-complete** \u2013 Type hints make editor tooling and docstrings clearer.\n",
    "3. **Runtime access** \u2013 Inside a tool, `ctx.deps` exposes that object alongside metadata such as `ctx.user_message` and `ctx.conversation`.\n",
    "\n",
    "We'll start with an in-memory dataset, but the same pattern works with network or database clients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deps-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ConciergeDeps(BaseModel):\n",
    "    events: Dict[str, List[str]]\n",
    "    restaurants: Dict[str, List[str]]\n",
    "\n",
    "    def lookup(self, city: str) -> dict:\n",
    "        key = city.lower()\n",
    "        return {\n",
    "            \"events\": self.events.get(key, []),\n",
    "            \"restaurants\": self.restaurants.get(key, []),\n",
    "        }\n",
    "\n",
    "berlin_data = ConciergeDeps(\n",
    "    events={\n",
    "        \"berlin\": [\"Saturday street food market in Kreuzberg\", \"Museum Island late-night opening\", \"Spree sunset boat cruise\"],\n",
    "    },\n",
    "    restaurants={\n",
    "        \"berlin\": [\"Five Elephant Coffee Roastery\", \"Markthalle Neun vendors\", \"Mustafas Gem\u00fcse Kebap\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import RunContext\n",
    "\n",
    "guide = Agent(\n",
    "    model=OpenAIModel(model=\"gpt-4o-mini\"),\n",
    "    system_prompt=(\n",
    "        \"You are a Berlin concierge. Use the `fetch_local_options` tool before planning a day so you stay factual. \"\n",
    "        \"Summaries should include morning, afternoon, and evening suggestions.\"\n",
    "    ),\n",
    "    deps_type=ConciergeDeps,\n",
    ")\n",
    "\n",
    "@guide.tool\n",
    "def fetch_local_options(ctx: RunContext[ConciergeDeps], city: str) -> dict:\n",
    "    \"\"\"Return curated restaurants and events for the requested city.\"\"\"\n",
    "    return ctx.deps.lookup(city)\n",
    "\n",
    "day_plan = guide.run_sync(\n",
    "    \"Plan a Saturday in Berlin for a family with teenagers. Include meals and activities.\",\n",
    "    deps=berlin_data,\n",
    ")\n",
    "print(day_plan.response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting tool calls\n",
    "Tool executions are recorded on the result. You can explore them interactively or via code:\n",
    "```python\n",
    "for call in day_plan.tool_calls:\n",
    "    print(call.name, call.arguments, call.response)\n",
    "```\n",
    "Each `ToolCall` matches the structure described in the [tooling reference](https://ai.pydantic.dev/tools/#tool-calls), including start/end timestamps and any raised errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for call in day_plan.tool_calls:\n",
    "    print(f'Tool \u2192 {call.name}')\n",
    "    print('Arguments:', call.arguments)\n",
    "    print('Response:', call.response)\n",
    "    print('-' * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`RunContext` offers more than dependencies. Explore `ctx.user_message`, `ctx.deps`, and `ctx.messages` (the running transcript) inside tools to tailor behavior\u2014e.g., short-circuiting repeat lookups. The [context section of the docs](https://ai.pydantic.dev/tools/#tool-context) lists every field you can rely on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-explain",
   "metadata": {},
   "source": [
    "During the run the model can call `fetch_local_options`. The framework handles JSON arguments, dependency injection, and logging. Tool outputs are automatically threaded back into the model's context so later messages can reference them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retry-logfire",
   "metadata": {},
   "source": [
    "## 3. Model retry + Logfire tracing\n",
    "Real traffic can be noisy: models may time out or return malformed JSON. [`RetryPolicy`](https://ai.pydantic.dev/agents/#retry-policy) replays a call with the same inputs. Combining it with Logfire gives you deep observability\u2014every attempt (and tool call) shows up in the Logfire UI.\n",
    "\n",
    "The snippet below mirrors the [Logfire integration guide](https://ai.pydantic.dev/logfire/). When you are ready to ship, set `send_to_logfire=True` and supply your project token so traces stream to the hosted dashboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retry-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logfire\n",
    "from pydantic_ai import RetryPolicy\n",
    "\n",
    "logfire.configure(send_to_logfire=False)  # keep traces local while experimenting\n",
    "\n",
    "retry_policy = RetryPolicy(max_attempts=3)\n",
    "\n",
    "with logfire.span(\"concierge-run\"):\n",
    "    winter_plan = guide.run_sync(\n",
    "        \"It's February and cold. Suggest an indoor-focused Berlin Saturday.\",\n",
    "        deps=berlin_data,\n",
    "        retry=retry_policy,\n",
    "    )\n",
    "\n",
    "print(winter_plan.response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming",
   "metadata": {},
   "source": [
    "## 4. Streaming replies\n",
    "LLMs can stream partial text so you don't block users. `run_stream_sync` returns an iterator; use `text_stream()` to yield deltas as soon as they arrive. The [streaming guide](https://ai.pydantic.dev/results/#streaming) shows additional helpers for partial structured results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "with guide.run_stream_sync(\n",
    "    \"Stream a warm two-sentence welcome for someone arriving in Berlin this evening.\",\n",
    "    deps=berlin_data,\n",
    ") as stream:\n",
    "    for token in stream.text_stream():\n",
    "        print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guardrails",
   "metadata": {},
   "source": [
    "## 5. Guardrails: validation + redaction\n",
    "Structured results reduce hallucinations. Pass a Pydantic model as `result_type` and the agent will coerce the response into that schema (retrying on validation errors). You can mark sensitive fields with `Redact` so that logs mask them automatically. See the [guardrails overview](https://ai.pydantic.dev/guardrails/) for more patterns like numeric ranges, enums, and partial validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guardrails-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "\n",
    "from pydantic import BaseModel, EmailStr, Field, PositiveInt\n",
    "from pydantic_ai import Redact\n",
    "\n",
    "class DinnerReservation(BaseModel):\n",
    "    guest_name: str = Field(max_length=60)\n",
    "    guest_email: EmailStr = Redact()\n",
    "    party_size: PositiveInt = Field(le=8)\n",
    "    preferred_time: Literal[\"17:00\", \"18:00\", \"19:00\", \"20:00\"]\n",
    "    special_requests: Optional[str] = Field(default=None, max_length=200)\n",
    "\n",
    "reservations = Agent(\n",
    "    model=OpenAIModel(model=\"gpt-4o-mini\"),\n",
    "    system_prompt=(\n",
    "        \"Extract structured dinner reservations from chatty guest messages. \"\n",
    "        \"If details are missing, make a polite best guess.\"\n",
    "    ),\n",
    "    result_type=DinnerReservation,\n",
    ")\n",
    "\n",
    "guest_message = \"\"\"\n",
    "Hi! I'm Alex visiting with 3 coworkers this Friday. Could you book something cozy around 7pm?\n",
    "My email is alex@example.com and we'd love a vegetarian-friendly spot.\n",
    "\"\"\"\n",
    "\n",
    "reservation = reservations.run_sync(guest_message)\n",
    "\n",
    "print(reservation.data.model_dump())\n",
    "print(\"Shown in logs as:\", reservation.data.model_dump(mode=\"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrap-up",
   "metadata": {},
   "source": [
    "Redacted fields still exist in `reservation.data`, but when Logfire (or standard logging) serializes the object the value is replaced with `***`. If the model fails validation (e.g., returns an invalid email), PydanticAI automatically retries up to the configured limit before surfacing a `ValidationError`.\n",
    "\n",
    "## 6. Putting it all together\n",
    "To adapt this concierge into your own project:\n",
    "1. Swap the dictionaries with a real API client or database dependency.\n",
    "2. Add more tools\u2014weather lookup, transit times, ticket booking\u2014and observe how the agent decides between them.\n",
    "3. Combine streaming with a UI (FastAPI, Streamlit) to deliver real-time concierge experiences.\n",
    "4. Turn on hosted Logfire to collect traces from production traffic.\n",
    "5. Explore advanced features like [parallel tool calls](https://ai.pydantic.dev/tools/#parallel-tools), [memory primitives](https://ai.pydantic.dev/memory/), and [response planning](https://ai.pydantic.dev/plans/) once you're comfortable with the basics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-super-agents (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
