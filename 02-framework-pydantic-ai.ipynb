{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# 02. Moving to a Framework: PydanticAI\n",
    "\n",
    "In tutorial 01 we wired up a bare-bones agent manually. This lesson upgrades the workflow by adopting **[PydanticAI](https://github.com/pydantic/pydantic-ai)**, a lightweight agent framework that keeps prompts, tools, and validation in one place. We'll keep the examples compact so you can focus on the core ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "learning-goals",
   "metadata": {},
   "source": [
    "## What you'll learn\n",
    "- How to bootstrap an agent with `Agent` and a hosted model.\n",
    "- How to register tools and inject dependencies that give your model superpowers.\n",
    "- How to add structured outputs, guardrails (validation + redaction), and retries.\n",
    "- How to stream responses and log the conversation with Logfire.\n",
    "- How to glue everything together in a realistic concierge use-case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup",
   "metadata": {},
   "source": [
    "## 0. Requirements\n",
    "1. Install dependencies locally (already present in this course environment). If you're on your own machine run:\n",
    "   ```bash\n",
    "   pip install pydantic-ai logfire ipykernel\n",
    "   ```\n",
    "2. Provide an API key for the provider you want to use (OpenAI works great). In a terminal run `export OPENAI_API_KEY=...` before launching Jupyter.\n",
    "3. Optional but recommended: create a free [Logfire](https://logfire.pydantic.dev/) account for rich traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environment-check",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    print(\"⚠️  Set the OPENAI_API_KEY environment variable before talking to a hosted model.\")\n",
    "else:\n",
    "    print(\"API key detected ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "## 1. Meet `Agent`\n",
    "PydanticAI wraps model calls inside an `Agent`. You configure the model, give it a concise system prompt, and call `run_sync` to execute a turn.\n",
    "\n",
    "We'll use a concierge persona throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "basic-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel\n",
    "\n",
    "concierge = Agent(\n",
    "    model=OpenAIModel(model=\"gpt-4o-mini\"),\n",
    "    system_prompt=\"You are a friendly city concierge. Keep replies short, positive, and specific to Berlin.\",\n",
    ")\n",
    "\n",
    "intro = concierge.run_sync(\"Welcome a visitor to Berlin in one sentence.\")\n",
    "print(intro.response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1-explain",
   "metadata": {},
   "source": [
    "`Agent` accepts the model backend (OpenAI, Anthropic, etc.), a system prompt, and optional extras. `run_sync` sends the user message, waits for the model, and returns a `RunResult` with metadata like `response_text`, tool traces, and tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "## 2. Tools + Dependencies: Local Concierge Use-Case\n",
    "Agents become useful once they can call real data. We'll model a concierge that knows Berlin restaurants and events stored in plain Python dictionaries.\n",
    "\n",
    "PydanticAI lets you declare a dependency schema (`deps_type`) and register tool functions with the `@agent.tool` decorator. Tools receive a `RunContext` that exposes the dependencies plus request metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deps-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ConciergeDeps(BaseModel):\n",
    "    events: Dict[str, List[str]]\n",
    "    restaurants: Dict[str, List[str]]\n",
    "\n",
    "    def lookup(self, city: str) -> dict:\n",
    "        key = city.lower()\n",
    "        return {\n",
    "            \"events\": self.events.get(key, []),\n",
    "            \"restaurants\": self.restaurants.get(key, []),\n",
    "        }\n",
    "\n",
    "berlin_data = ConciergeDeps(\n",
    "    events={\n",
    "        \"berlin\": [\"Saturday street food market in Kreuzberg\", \"Museum Island late-night opening\", \"Spree sunset boat cruise\"],\n",
    "    },\n",
    "    restaurants={\n",
    "        \"berlin\": [\"Five Elephant Coffee Roastery\", \"Markthalle Neun vendors\", \"Mustafas Gemüse Kebap\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tool-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import RunContext\n",
    "\n",
    "guide = Agent(\n",
    "    model=OpenAIModel(model=\"gpt-4o-mini\"),\n",
    "    system_prompt=(\n",
    "        \"You are a Berlin concierge. Use the `fetch_local_options` tool before planning a day so you stay factual. \"\n",
    "        \"Summaries should include morning, afternoon, and evening suggestions.\"\n",
    "    ),\n",
    "    deps_type=ConciergeDeps,\n",
    ")\n",
    "\n",
    "@guide.tool\n",
    "def fetch_local_options(ctx: RunContext[ConciergeDeps], city: str) -> dict:\n",
    "    \"\"\"Return curated restaurants and events for the requested city.\"\"\"\n",
    "    return ctx.deps.lookup(city)\n",
    "\n",
    "day_plan = guide.run_sync(\n",
    "    \"Plan a Saturday in Berlin for a family with teenagers. Include meals and activities.\",\n",
    "    deps=berlin_data,\n",
    ")\n",
    "print(day_plan.response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tools-explain",
   "metadata": {},
   "source": [
    "During the run the model can call `fetch_local_options`. The framework handles JSON arguments, dependency injection, and logging. Tool outputs are automatically threaded back into the model's context so later messages can reference them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retry-logfire",
   "metadata": {},
   "source": [
    "## 3. Model retry + Logfire tracing\n",
    "Real traffic can be noisy: models may time out or return malformed JSON. `RetryPolicy` replays a call with the same inputs. Combining it with Logfire gives you deep observability—every attempt (and tool call) shows up in the Logfire UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retry-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logfire\n",
    "from pydantic_ai import RetryPolicy\n",
    "\n",
    "logfire.configure(send_to_logfire=False)  # keep traces local while experimenting\n",
    "\n",
    "retry_policy = RetryPolicy(max_attempts=3)\n",
    "\n",
    "with logfire.span(\"concierge-run\"):\n",
    "    winter_plan = guide.run_sync(\n",
    "        \"It's February and cold. Suggest an indoor-focused Berlin Saturday.\",\n",
    "        deps=berlin_data,\n",
    "        retry=retry_policy,\n",
    "    )\n",
    "\n",
    "print(winter_plan.response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming",
   "metadata": {},
   "source": [
    "## 4. Streaming replies\n",
    "LLMs can stream partial text so you don't block users. `run_stream_sync` returns an iterator; use `text_stream()` to yield deltas as soon as they arrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "with guide.run_stream_sync(\n",
    "    \"Stream a warm two-sentence welcome for someone arriving in Berlin this evening.\",\n",
    "    deps=berlin_data,\n",
    ") as stream:\n",
    "    for token in stream.text_stream():\n",
    "        print(token, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guardrails",
   "metadata": {},
   "source": [
    "## 5. Guardrails: validation + redaction\n",
    "Structured results reduce hallucinations. Pass a Pydantic model as `result_type` and the agent will coerce the response into that schema (retrying on validation errors). You can mark sensitive fields with `Redact` so that logs mask them automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guardrails-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Optional\n",
    "\n",
    "from pydantic import BaseModel, EmailStr, Field, PositiveInt\n",
    "from pydantic_ai import Redact\n",
    "\n",
    "class DinnerReservation(BaseModel):\n",
    "    guest_name: str = Field(max_length=60)\n",
    "    guest_email: EmailStr = Redact()\n",
    "    party_size: PositiveInt = Field(le=8)\n",
    "    preferred_time: Literal[\"17:00\", \"18:00\", \"19:00\", \"20:00\"]\n",
    "    special_requests: Optional[str] = Field(default=None, max_length=200)\n",
    "\n",
    "reservations = Agent(\n",
    "    model=OpenAIModel(model=\"gpt-4o-mini\"),\n",
    "    system_prompt=(\n",
    "        \"Extract structured dinner reservations from chatty guest messages. \"\n",
    "        \"If details are missing, make a polite best guess.\"\n",
    "    ),\n",
    "    result_type=DinnerReservation,\n",
    ")\n",
    "\n",
    "guest_message = \"\"\"\n",
    "Hi! I'm Alex visiting with 3 coworkers this Friday. Could you book something cozy around 7pm?\n",
    "My email is alex@example.com and we'd love a vegetarian-friendly spot.\n",
    "\"\"\"\n",
    "\n",
    "reservation = reservations.run_sync(guest_message)\n",
    "\n",
    "print(reservation.data.model_dump())\n",
    "print(\"Shown in logs as:\", reservation.data.model_dump(mode=\"json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrap-up",
   "metadata": {},
   "source": [
    "Redacted fields still exist in `reservation.data`, but when Logfire (or standard logging) serializes the object the value is replaced with `***`.\n",
    "\n",
    "## 6. Next steps\n",
    "- Swap the dictionaries with a real API client or database dependency.\n",
    "- Add more tools—weather lookup, transit times, ticket booking—and observe how the agent decides between them.\n",
    "- Combine streaming with a UI (FastAPI, Streamlit) to deliver real-time concierge experiences.\n",
    "- Explore advanced features like parallel tool calls, memory stores, and evaluation notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-super-agents (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
