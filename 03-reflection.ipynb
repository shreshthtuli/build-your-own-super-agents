{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7ed5baf",
   "metadata": {},
   "source": [
    "# 03. Reflection and Self-Improvement Loops\n",
    "\n",
    "This tutorial dives into reflection-driven workflows for language-model agents. We survey the research foundations, implement multiple critique strategies, and compare how each method improves a realistic customer-support scenario. The goal is to build intuition for when to keep prompts simple, when to add structured self-reflection, and how to layer multiple models for critique, debate, or automated prompt editing. Let's see:\n",
    "\n",
    "- Why reflection is a core building block in modern agent stacks.\n",
    "- Theoretical grounding for self-critique, debate, tree-of-thought search, and automated prompt editing.\n",
    "- How to implement reflection with and without external feedback signals.\n",
    "- How zero-shot and few-shot prompting compare against reflective variants on a realistic task.\n",
    "- Practical tips for choosing critique models, iteration budgets, and orchestration patterns.\n",
    "\n",
    "## Research foundations\n",
    "\n",
    "Reflection strategies for language models gained momentum with works like **Self-Consistency** [(Wang et al., 2022)](https://arxiv.org/pdf/2203.11171), **Chain-of-Thought** [(Wei et al., 2022)](https://arxiv.org/pdf/2201.11903), **Reflexion** [(Shinn et al., 2023)](https://arxiv.org/pdf/2303.11366), **Self-Refine** [(Madaan et al., 2023)](https://arxiv.org/pdf/2303.17651), **Multi-Agent Debate** [(Du et al., 2023)](https://arxiv.org/pdf/2305.14325), and **Tree-of-Thought** [(Yao et al., 2023)](https://arxiv.org/pdf/2305.10601). Each paper studies how structured critique boosts accuracy, reliability, and transparency compared to single-pass generation. Modern agents often mix these ideas: generate an initial answer, critique it (internally or with a helper model), incorporate new evidence, then revise the plan or response.\n",
    "\n",
    "### Map of reflection techniques\n",
    "\n",
    "| Technique | Core idea | Signals used | Classic references |\n",
    "| --- | --- | --- | --- |\n",
    "| **Self-critique loop** | Model critiques and revises its own answer iteratively. | Internal reasoning only. | Reflexion; Self-Refine. |\n",
    "| **Automated debate** | Multiple agents argue for different answers; a judge picks the best. | Internal plus comparative reasoning. | Multi-Agent Debate; Constitutional AI. |\n",
    "| **Tree-of-Thought (ToT)** | Expand a reasoning tree, evaluate branches, and pick the best path. | Internal reasoning, heuristic scoring. | Tree-of-Thought; Least-to-Most Prompting. |\n",
    "| **Prompt editing** | Improve prompts automatically based on evaluation metrics. | Internal evaluation signals; can ingest external metrics. | PromptAgent; DSPy. |\n",
    "| **External feedback loop** | Incorporate retrieved facts, tool outputs, or human critiques. | External signals (APIs, users, knowledge bases). | RAG triads; Reflexion with environment rewards. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9648c2dd",
   "metadata": {},
   "source": [
    "## Reflection without external inputs\n",
    "\n",
    "When we only have a language model and no extra evidence, we rely on its latent knowledge. Reflection works by asking the model to reason step-by-step, criticize mistakes, and iterate. Key tools include:\n",
    "\n",
    "1. **Self-critique loops:** Draft → critique → revise. Reflexion and Self-Refine demonstrate that even a single iteration can close 10–40% of the performance gap to supervised fine-tuning on reasoning tasks.\n",
    "2. **Tree-of-thought exploration:** Instead of a single chain-of-thought, generate multiple reasoning branches, score them, and keep the most promising path. Yao et al. show ToT helps puzzle solving and long math proofs.\n",
    "3. **Automated prompt editing:** Rather than rewriting outputs, rewrite the prompt. Systems like PromptAgent and DSPy treat prompt tokens as parameters optimized by heuristic feedback.\n",
    "\n",
    "These methods add negligible infrastructure yet unlock large gains when the model already “knows” the answer but needs help organizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5097b2",
   "metadata": {},
   "source": [
    "### Self-critique loops in practice\n",
    "\n",
    "Core pattern:\n",
    "\n",
    "1. Produce an initial answer with minimal guardrails.\n",
    "2. Ask for a critique that lists factual gaps, style issues, or constraint violations.\n",
    "3. Feed the critique back to the model (or a separate reviser) to update the answer.\n",
    "4. Optionally repeat until improvements plateau or a budget is exhausted.\n",
    "\n",
    "Benefits: cheap, improves recall, enforces structure. Risks: looping forever or over-correcting. Use a max-iteration guard and keep critiques focused (e.g., “List missing requirements in bullet form”)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71035a23",
   "metadata": {},
   "source": [
    "### Tree-of-thought search\n",
    "\n",
    "Tree-of-thought (ToT) builds a search tree where each node is a reasoning state. We expand several branches in parallel, score them (via heuristics or the model itself), and continue with the best. Compared to linear chain-of-thought:\n",
    "\n",
    "- Encourages exploration before committing to a final answer.\n",
    "- Helps long-horizon planning and combinatorial tasks (math, code synthesis, scheduling).\n",
    "- Requires orchestration logic to manage breadth/depth and avoid exponential blow-up.\n",
    "\n",
    "This tutorial implements a lightweight ToT that expands two candidate thoughts, scores them using a heuristic (coverage of customer needs), and finalizes a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b3df9",
   "metadata": {},
   "source": [
    "### Automated prompt editing\n",
    "\n",
    "Prompt editing treats the prompt as a program. After evaluating outputs, we update the prompt to address systematic issues (missing disclaimers, tone mismatches, etc.). DSPy [(Khattab et al., 2024)](https://arxiv.org/pdf/2310.03714) automates this by exposing prompt fields as parameters optimized by feedback functions. In production, feedback can include precision/recall metrics, policy checkers, or human votes. We will implement a basic prompt editor that:\n",
    "\n",
    "1. Scores responses.\n",
    "2. Detects the most common missing key point.\n",
    "3. Appends a targeted instruction to the prompt template.\n",
    "4. Re-generates answers with the refined prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30382c6f",
   "metadata": {},
   "source": [
    "## Reflection with external inputs\n",
    "\n",
    "Real-world agents rarely reason in isolation. Reflection can incorporate external data:\n",
    "\n",
    "- **Retrieved evidence:** Use a retrieval-augmented pipeline to ground the critique (e.g., cite knowledge base articles). Reflexion with environment rewards uses tool outputs as signals.\n",
    "- **Telemetry or analytics:** Customer-facing agents monitor success metrics (resolution rates, CSAT). Failures feed a reflection loop that updates prompts or tool sequences.\n",
    "- **Human-in-the-loop critiques:** Constitutional AI and reinforcement learning from AI feedback (RLAIF) employ helper models or human judges to supply critiques that shape revisions.\n",
    "\n",
    "Our coding example stays offline but highlights where external signals would plug into each loop (e.g., replacing heuristic coverage with actual QA metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29cab9",
   "metadata": {},
   "source": [
    "## Choosing models for generation and critique\n",
    "\n",
    "A common pattern is to dedicate smaller, cheaper models to critique while keeping a stronger (possibly more expensive) model for the final revision. For example:\n",
    "\n",
    "| Role | Recommended model tier | Notes |\n",
    "| --- | --- | --- |\n",
    "| Draft generator | Capable general model (GPT-4, Claude Opus, Gemini Pro) | Needs reasoning and domain knowledge. |\n",
    "| Critic / Judge | Balanced model (GPT-4o mini, Claude Sonnet, Llama-3 70B) | Precision matters more than creativity. |\n",
    "| Tool caller | Lightweight function-calling model | Focus on reliability and structured outputs. |\n",
    "| Prompt editor | Smaller yet controllable model | Produces instructions; does not need creative flair. |\n",
    "\n",
    "When budgets are tight, you can reuse the same model for multiple roles but limit iterations. For safety-critical domains, keep the critic stronger or more policy-aligned than the generator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb00cb83",
   "metadata": {},
   "source": [
    "## Zero-shot vs. few-shot vs. reflective prompting\n",
    "\n",
    "- **Zero-shot** relies entirely on instructions in the prompt. Fast but brittle: if the model misses a requirement, nothing pulls it back.\n",
    "- **Few-shot** shows the model labeled examples. This improves format adherence but still lacks a correction loop.\n",
    "- **Reflective methods** iterate. They recover from omissions, enforce domain policies, and maintain quality over long conversations.\n",
    "\n",
    "We will quantify these differences using a simulated customer-support workload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccc7e90",
   "metadata": {},
   "source": [
    "## Scenario: OrbitServe customer support\n",
    "\n",
    "Imagine you operate *OrbitServe*, a SaaS analytics platform. Support agents must follow policy checklists: acknowledge the issue, state mitigation steps, provide ETAs, and include compliance language. We will craft four recent tickets and evaluate how different prompting/reflection strategies handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003dcc73",
   "metadata": {},
   "source": [
    "### Policy checklist\n",
    "\n",
    "Each ticket includes `key_points` that a good response must cover. These act as automatic evaluation criteria similar to recall checks or rubric scoring in production QA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba774aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker: gpt-4o-mini | Critic/Judge: gpt-4o\n"
     ]
    }
   ],
   "source": [
    "import os, time, random, re, math\n",
    "from typing import List, Literal, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_ai import Agent\n",
    "from pydantic_ai.models.openai import OpenAIModel  # works for OpenAI or any OpenAI-compatible endpoint\n",
    "\n",
    "# Models (tune as you like)\n",
    "DEFAULT_MODEL_ID = os.getenv(\"LLM_MODEL\",  \"gpt-4o-mini\")\n",
    "CRITIC_MODEL_ID  = os.getenv(\"CRITIC_MODEL\",\"gpt-4o\")  # stronger judge by default\n",
    "\n",
    "print(\"Worker:\", DEFAULT_MODEL_ID, \"| Critic/Judge:\", CRITIC_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89236c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 7\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "SORTING_EVAL_PATH = \"data/sorting_eval.jsonl\"\n",
    "SENTIMENT_EVAL_PATH = \"data/sentiment_eval.jsonl\"\n",
    "\n",
    "if os.path.exists(\"data/sorting_eval.jsonl\"):\n",
    "    sort_items = json.loads(da)\n",
    "\n",
    "sort_items = [\n",
    "    {\"inp\": [5, 1, 4], \"out\": [1, 4, 5]},\n",
    "    {\"inp\": [3.2, 3.1, -1], \"out\": [-1, 3.1, 3.2]},\n",
    "    {\"inp\": [10, 7, 7, 8], \"out\": [7, 7, 8, 10]},\n",
    "    {\"inp\": [0, -5, 2], \"out\": [-5, 0, 2]},\n",
    "]\n",
    "\n",
    "# Sentiment task\n",
    "sentiment_items = [\n",
    "    {\"text\": \"Love the UI, super smooth!\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Terrible lag and frequent crashes.\", \"label\": \"negative\"},\n",
    "    {\"text\": \"Pretty good overall, but the ads are annoying.\", \"label\": \"positive\"},\n",
    "    {\"text\": \"Bad update. The app freezes on login.\", \"label\": \"negative\"},\n",
    "]\n",
    "\n",
    "def shuffled(xs):\n",
    "    xs = xs[:]\n",
    "    random.shuffle(xs)\n",
    "    return xs\n",
    "\n",
    "sort_eval = shuffled(sort_items)\n",
    "sent_eval = shuffled(sentiment_items)\n",
    "\n",
    "@dataclass\n",
    "class RunResult:\n",
    "    accuracy: float\n",
    "    latency_s: float\n",
    "    raw: List\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5357e1f",
   "metadata": {},
   "source": [
    "We will evaluate each response against these checklists and compute coverage scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc8c130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def coverage_score(text: str, key_points: List[str]) -> float:\n",
    "    text_lower = text.lower()\n",
    "    hits = sum(1 for phrase in key_points if phrase.lower() in text_lower)\n",
    "    return hits / len(key_points)\n",
    "\n",
    "\n",
    "def score_responses(responses: Dict[int, str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for ticket in TICKETS:\n",
    "        reply = responses.get(ticket.id, \"\")\n",
    "        cov = coverage_score(reply, ticket.key_points)\n",
    "        rows.append(\n",
    "            {\n",
    "                \"ticket\": ticket.id,\n",
    "                \"customer\": ticket.customer,\n",
    "                \"coverage\": round(cov, 2),\n",
    "                \"length\": len(reply.split()),\n",
    "                \"missing\": [kp for kp in ticket.key_points if kp.lower() not in reply.lower()],\n",
    "            }\n",
    "        )\n",
    "    return pd.DataFrame(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce0b36e",
   "metadata": {},
   "source": [
    "`coverage_score` approximates how many policy checkpoints appear in the reply. Real systems often use rubric graders, QA teams, or automated policy classifiers. We also log response length and which key points remain missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb2af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import field\n",
    "\n",
    "@dataclass\n",
    "class FakeLLM:\n",
    "    name: str\n",
    "    scripts: Dict[str, str]\n",
    "\n",
    "    def complete(self, prompt: str) -> str:\n",
    "        for key, value in self.scripts.items():\n",
    "            if key in prompt:\n",
    "                return value\n",
    "        return self.scripts.get(\"default\", \"I am unsure.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0c5b00",
   "metadata": {},
   "source": [
    "### Baseline models\n",
    "\n",
    "We start with two basic prompting strategies:\n",
    "\n",
    "1. **Zero-shot:** a single instruction with no examples.\n",
    "2. **Few-shot:** two policy-compliant exemplars before the new ticket.\n",
    "\n",
    "The `FakeLLM` scripts below simulate how a production model might respond under each setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbb9312",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_model = FakeLLM(\n",
    "    name=\"ZeroShotGPT\",\n",
    "    scripts={\n",
    "        \"[MODE:ZERO_SHOT][TICKET:1]\": \"Hi HelioMart, we saw the errors and will investigate soon. Thanks for your patience.\",\n",
    "        \"[MODE:ZERO_SHOT][TICKET:2]\": \"Hello NimbusBank team, the export should be fine. Please check the portal.\",\n",
    "        \"[MODE:ZERO_SHOT][TICKET:3]\": \"Hi Wayfinder, the webhooks are being checked by engineering.\",\n",
    "        \"[MODE:ZERO_SHOT][TICKET:4]\": \"Aurora Health, security remains a top priority and we comply with HIPAA.\",\n",
    "        \"default\": \"Let me look into that for you.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "few_shot_model = FakeLLM(\n",
    "    name=\"FewShotGPT\",\n",
    "    scripts={\n",
    "        \"[MODE:FEW_SHOT][TICKET:1]\": (\n",
    "            \"Hello HelioMart, apologies for the disruption. Our engineers are restoring analytics and expect completion in about two hours.\"\n",
    "            \" In the meantime you can export CSVs from Settings > Data Export.\"\n",
    "        ),\n",
    "        \"[MODE:FEW_SHOT][TICKET:2]\": (\n",
    "            \"Hi NimbusBank, confirming the February SOC 2 export is signed with hash 9f2b-aa14.\"\n",
    "            \" Use Compliance Hub > Audits to download after MFA and let us know if auditors need help.\"\n",
    "        ),\n",
    "        \"[MODE:FEW_SHOT][TICKET:3]\": (\n",
    "            \"Wayfinder team, thanks for flagging the deploy regression. The retry queue drained but engineering is monitoring.\"\n",
    "        ),\n",
    "        \"[MODE:FEW_SHOT][TICKET:4]\": (\n",
    "            \"Aurora Health, PHI stays encrypted and we limit support access.\"),\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd1cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_zero_shot() -> Dict[int, str]:\n",
    "    responses = {}\n",
    "    for ticket in TICKETS:\n",
    "        prompt = f\"[MODE:ZERO_SHOT][TICKET:{ticket.id}] {ticket.issue}\"\n",
    "        responses[ticket.id] = zero_shot_model.complete(prompt)\n",
    "    return responses\n",
    "\n",
    "\n",
    "def run_few_shot() -> Dict[int, str]:\n",
    "    responses = {}\n",
    "    for ticket in TICKETS:\n",
    "        prompt = f\"[MODE:FEW_SHOT][TICKET:{ticket.id}] {ticket.issue}\"\n",
    "        responses[ticket.id] = few_shot_model.complete(prompt)\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431ea049",
   "metadata": {},
   "source": [
    "### Baseline performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21dd519",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_shot_scores = score_responses(run_zero_shot())\n",
    "few_shot_scores = score_responses(run_few_shot())\n",
    "\n",
    "baseline_summary = pd.concat([\n",
    "    zero_shot_scores.assign(method=\"Zero-shot\"),\n",
    "    few_shot_scores.assign(method=\"Few-shot\"),\n",
    "])\n",
    "\n",
    "baseline_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978777ff",
   "metadata": {},
   "source": [
    "Zero-shot prompting barely hits the required checkpoints. Few-shot improves tone and hits some policies, but still misses critical items (e.g., workaround instructions, explicit ETAs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539eec64",
   "metadata": {},
   "source": [
    "## Implementing self-critique loops\n",
    "\n",
    "We now implement a Reflexion-style loop with separate generator, critic, and reviser models. The critic enumerates missing key points and tone issues; the reviser integrates that feedback into a new answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e08fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_critique_generator = FakeLLM(\n",
    "    name=\"SelfCritiqueGenerator\",\n",
    "    scripts={\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:answer][TICKET:1]\": (\n",
    "            \"HelioMart team, sorry about the outage. Engineering is on it and will update soon.\"\n",
    "        ),\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:answer][TICKET:2]\": (\n",
    "            \"NimbusBank, the audit export should still be valid; please re-download.\"\n",
    "        ),\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:answer][TICKET:3]\": (\n",
    "            \"Wayfinder, we noticed webhook retries paused and are investigating.\"\n",
    "        ),\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:answer][TICKET:4]\": (\n",
    "            \"Aurora Health, we keep PHI protected during reviews.\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "self_critique_critic = FakeLLM(\n",
    "    name=\"SelfCritiqueCritic\",\n",
    "    scripts={\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:critique][TICKET:1]\": (\n",
    "            \"Missing: confirm engineers are restoring analytics, give eta 2 hours, offer csv export workaround.\"\n",
    "        ),\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:critique][TICKET:2]\": (\n",
    "            \"Missing: reference SOC 2 signed export with hash, outline secure download steps, invite follow-up.\"\n",
    "        ),\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:critique][TICKET:3]\": (\n",
    "            \"Missing: acknowledge deploy regression explicitly, cite retry queue draining, advise CLI replay, escalate to on-call SRE.\"\n",
    "        ),\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:critique][TICKET:4]\": (\n",
    "            \"Missing: affirm PHI encryption, describe just-in-time access controls, note audit logging retention, share HIPAA knowledge base link.\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "self_critique_reviser = FakeLLM(\n",
    "    name=\"SelfCritiqueReviser\",\n",
    "    scripts={\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:revise][TICKET:1]\": (\n",
    "            \"Hi HelioMart, apologies for the disruption. Our engineers are actively restoring analytics and expect full recovery in about eta 2 hours.\"\n",
    "            \" While we finish the fix you can offer csv export workaround from Settings > Data Export.\"\n",
    "        ),\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:revise][TICKET:2]\": (\n",
    "            \"Hello NimbusBank, confirming the February SOC 2 signed export is available with signature hash 9f2b-aa14.\"\n",
    "            \" After MFA, follow the Compliance Hub download steps and please invite follow-up if anything looks off.\"\n",
    "        ),\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:revise][TICKET:3]\": (\n",
    "            \"Wayfinder team, acknowledging the deploy regression that paused retries. The retry queue draining was expected but events are pending.\"\n",
    "            \" Please advise requeue via CLI using `orbitserve webhooks replay --since=24h` while we escalate to on-call SRE.\"\n",
    "        ),\n",
    "        \"[MODE:SELF_CRITIQUE][STEP:revise][TICKET:4]\": (\n",
    "            \"Aurora Health, affirming PHI encryption at rest and in transit. Support uses just-in-time access controls with dual approval,\"\n",
    "            \" and every access is logged with retention for audit logging retention of seven years.\"\n",
    "            \" Here is the HIPAA knowledge base link: https://kb.orbitserve.com/hipaa-privacy-411.\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "class SelfCritiqueAgent:\n",
    "    def __init__(self, generator: FakeLLM, critic: FakeLLM, reviser: FakeLLM, max_iterations: int = 1):\n",
    "        self.generator = generator\n",
    "        self.critic = critic\n",
    "        self.reviser = reviser\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def respond(self, ticket: Ticket) -> Tuple[str, Dict[str, str]]:\n",
    "        draft_prompt = f\"[MODE:SELF_CRITIQUE][STEP:answer][TICKET:{ticket.id}] {ticket.issue}\"\n",
    "        draft = self.generator.complete(draft_prompt)\n",
    "\n",
    "        critique_prompt = f\"[MODE:SELF_CRITIQUE][STEP:critique][TICKET:{ticket.id}] Draft: {draft}\"\n",
    "        critique = self.critic.complete(critique_prompt)\n",
    "\n",
    "        revise_prompt = (\n",
    "            f\"[MODE:SELF_CRITIQUE][STEP:revise][TICKET:{ticket.id}] Draft: {draft}\n",
    "Critique: {critique}\"\n",
    "        )\n",
    "        revision = self.reviser.complete(revise_prompt)\n",
    "\n",
    "        return revision, {\"draft\": draft, \"critique\": critique}\n",
    "\n",
    "\n",
    "self_critique_agent = SelfCritiqueAgent(\n",
    "    generator=self_critique_generator,\n",
    "    critic=self_critique_critic,\n",
    "    reviser=self_critique_reviser,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b513fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_critique() -> Tuple[Dict[int, str], Dict[int, Dict[str, str]]]:\n",
    "    responses = {}\n",
    "    traces = {}\n",
    "    for ticket in TICKETS:\n",
    "        reply, trace = self_critique_agent.respond(ticket)\n",
    "        responses[ticket.id] = reply\n",
    "        traces[ticket.id] = trace\n",
    "    return responses, traces\n",
    "\n",
    "\n",
    "self_critique_responses, self_critique_traces = run_self_critique()\n",
    "self_critique_scores = score_responses(self_critique_responses)\n",
    "self_critique_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e069901",
   "metadata": {},
   "source": [
    "Self-critique covers every policy checkpoint while keeping responses concise. You can inspect the `self_critique_traces` dictionary to see drafts and critiques—useful for telemetry dashboards in production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2727c44c",
   "metadata": {},
   "source": [
    "## Multi-agent debate\n",
    "\n",
    "Debate-based systems create two (or more) agents that argue for competing answers. A judge model evaluates their reasoning and selects or synthesizes the final reply. Debate reduces individual hallucinations and encourages thorough justification. We simulate a two-agent debate with a neutral judge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd453e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "debate_agent_a = FakeLLM(\n",
    "    name=\"DebaterA\",\n",
    "    scripts={\n",
    "        \"[MODE:DEBATE][AGENT:A][TICKET:1]\": \"Argument A: Mention engineers restoring analytics and add eta 2 hours.\",\n",
    "        \"[MODE:DEBATE][AGENT:A][TICKET:2]\": \"Argument A: Stress SOC 2 signed export and cite hash.\",\n",
    "        \"[MODE:DEBATE][AGENT:A][TICKET:3]\": \"Argument A: Highlight retry queue draining and need for CLI replay.\",\n",
    "        \"[MODE:DEBATE][AGENT:A][TICKET:4]\": \"Argument A: Emphasize PHI encryption and audit logs.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "debate_agent_b = FakeLLM(\n",
    "    name=\"DebaterB\",\n",
    "    scripts={\n",
    "        \"[MODE:DEBATE][AGENT:B][TICKET:1]\": \"Argument B: Recommend csv export workaround until fix completes.\",\n",
    "        \"[MODE:DEBATE][AGENT:B][TICKET:2]\": \"Argument B: Provide download steps and invite follow-up questions.\",\n",
    "        \"[MODE:DEBATE][AGENT:B][TICKET:3]\": \"Argument B: Escalate to on-call SRE and reassure monitoring.\",\n",
    "        \"[MODE:DEBATE][AGENT:B][TICKET:4]\": \"Argument B: Share HIPAA knowledge base link and JIT access controls.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "debate_judge = FakeLLM(\n",
    "    name=\"DebateJudge\",\n",
    "    scripts={\n",
    "        \"[MODE:DEBATE][ROLE:JUDGE][TICKET:1]\": (\n",
    "            \"Final: Apologize, confirm engineers restoring analytics with eta 2 hours, and advise csv export workaround.\"\n",
    "        ),\n",
    "        \"[MODE:DEBATE][ROLE:JUDGE][TICKET:2]\": (\n",
    "            \"Final: Confirm SOC 2 signed export with hash, describe secure download steps, and invite follow-up.\"\n",
    "        ),\n",
    "        \"[MODE:DEBATE][ROLE:JUDGE][TICKET:3]\": (\n",
    "            \"Final: Acknowledge deploy regression, note retry queue draining, advise CLI replay, and escalate to on-call SRE.\"\n",
    "        ),\n",
    "        \"[MODE:DEBATE][ROLE:JUDGE][TICKET:4]\": (\n",
    "            \"Final: Affirm PHI encryption, outline JIT access controls, note audit logging retention, share HIPAA knowledge base link.\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "class DebateOrchestrator:\n",
    "    def __init__(self, agent_a: FakeLLM, agent_b: FakeLLM, judge: FakeLLM):\n",
    "        self.agent_a = agent_a\n",
    "        self.agent_b = agent_b\n",
    "        self.judge = judge\n",
    "\n",
    "    def respond(self, ticket: Ticket) -> Dict[str, str]:\n",
    "        prompt_a = f\"[MODE:DEBATE][AGENT:A][TICKET:{ticket.id}] {ticket.issue}\"\n",
    "        prompt_b = f\"[MODE:DEBATE][AGENT:B][TICKET:{ticket.id}] {ticket.issue}\"\n",
    "        argument_a = self.agent_a.complete(prompt_a)\n",
    "        argument_b = self.agent_b.complete(prompt_b)\n",
    "\n",
    "        judge_prompt = (\n",
    "            f\"[MODE:DEBATE][ROLE:JUDGE][TICKET:{ticket.id}]\"\n",
    "            f\" Arguments: {argument_a} || {argument_b}\"\n",
    "        )\n",
    "        decision = self.judge.complete(judge_prompt)\n",
    "\n",
    "        return {\n",
    "            \"argument_a\": argument_a,\n",
    "            \"argument_b\": argument_b,\n",
    "            \"final\": decision.replace(\"Final: \", \"\"),\n",
    "        }\n",
    "\n",
    "\n",
    "debate_orchestrator = DebateOrchestrator(debate_agent_a, debate_agent_b, debate_judge)\n",
    "\n",
    "\n",
    "def run_debate() -> Tuple[Dict[int, str], Dict[int, Dict[str, str]]]:\n",
    "    responses = {}\n",
    "    traces = {}\n",
    "    for ticket in TICKETS:\n",
    "        result = debate_orchestrator.respond(ticket)\n",
    "        responses[ticket.id] = result[\"final\"]\n",
    "        traces[ticket.id] = result\n",
    "    return responses, traces\n",
    "\n",
    "\n",
    "debate_responses, debate_traces = run_debate()\n",
    "debate_scores = score_responses(debate_responses)\n",
    "debate_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd7f350",
   "metadata": {},
   "source": [
    "Debate reaches perfect coverage like self-critique, but the trace now includes two rationales—handy for post-mortems or red-teaming. Debate is more expensive (multiple model calls) yet helpful when single-agent critiques miss contradictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7e9d98",
   "metadata": {},
   "source": [
    "## Tree-of-thought reflection\n",
    "\n",
    "We now model a shallow tree-of-thought process. The planner proposes two candidate plans (“thoughts”), a scorer picks the better one using a heuristic, and the finalizer turns the winning plan into a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e244266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_planner = FakeLLM(\n",
    "    name=\"ToTPlanner\",\n",
    "    scripts={\n",
    "        \"[MODE:TOT][STEP:PLAN][TICKET:1]\": \"Thought 1: Apologize + engineers fixing + eta 2 hours.\n",
    "Thought 2: Focus on workaround only.\",\n",
    "        \"[MODE:TOT][STEP:PLAN][TICKET:2]\": \"Thought 1: Detail SOC 2 signed export steps.\n",
    "Thought 2: Send generic reassurance.\",\n",
    "        \"[MODE:TOT][STEP:PLAN][TICKET:3]\": \"Thought 1: Mention deploy regression, CLI replay, SRE escalation.\n",
    "Thought 2: Wait for monitoring.\",\n",
    "        \"[MODE:TOT][STEP:PLAN][TICKET:4]\": \"Thought 1: Explain PHI encryption, JIT access, audit logs, KB link.\n",
    "Thought 2: Say compliance is taken seriously.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "tot_scorer = FakeLLM(\n",
    "    name=\"ToTScorer\",\n",
    "    scripts={\n",
    "        \"[MODE:TOT][STEP:SCORE][TICKET:1]\": \"Score: Thought 1 hits more customer needs.\",\n",
    "        \"[MODE:TOT][STEP:SCORE][TICKET:2]\": \"Score: Thought 1 mentions signed export and download steps.\",\n",
    "        \"[MODE:TOT][STEP:SCORE][TICKET:3]\": \"Score: Thought 1 covers regression and CLI replay requirements.\",\n",
    "        \"[MODE:TOT][STEP:SCORE][TICKET:4]\": \"Score: Thought 1 covers encryption, access controls, audit logging, KB link.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "tot_finalizer = FakeLLM(\n",
    "    name=\"ToTFinalizer\",\n",
    "    scripts={\n",
    "        \"[MODE:TOT][STEP:FINAL][CHOICE:1][TICKET:1]\": (\n",
    "            \"HelioMart, apologize for the disruption; engineers are restoring analytics with eta 2 hours.\"\n",
    "            \" Offer csv export workaround until dashboards recover.\"\n",
    "        ),\n",
    "        \"[MODE:TOT][STEP:FINAL][CHOICE:1][TICKET:2]\": (\n",
    "            \"NimbusBank, reference SOC 2 signed export with signature hash 9f2b-aa14, outline secure download steps, and invite follow-up.\"\n",
    "        ),\n",
    "        \"[MODE:TOT][STEP:FINAL][CHOICE:1][TICKET:3]\": (\n",
    "            \"Wayfinder, acknowledge deploy regression, cite retry queue draining, advise requeue via CLI, and escalate to on-call SRE.\"\n",
    "        ),\n",
    "        \"[MODE:TOT][STEP:FINAL][CHOICE:1][TICKET:4]\": (\n",
    "            \"Aurora Health, affirm PHI encryption, describe just-in-time access controls, note audit logging retention, share HIPAA knowledge base link.\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "class TreeOfThoughtAgent:\n",
    "    def __init__(self, planner: FakeLLM, scorer: FakeLLM, finalizer: FakeLLM):\n",
    "        self.planner = planner\n",
    "        self.scorer = scorer\n",
    "        self.finalizer = finalizer\n",
    "\n",
    "    def respond(self, ticket: Ticket) -> Dict[str, str]:\n",
    "        plan_prompt = f\"[MODE:TOT][STEP:PLAN][TICKET:{ticket.id}] {ticket.issue}\"\n",
    "        thoughts = self.planner.complete(plan_prompt).splitlines()\n",
    "        score_prompt = f\"[MODE:TOT][STEP:SCORE][TICKET:{ticket.id}] Thoughts: {' || '.join(thoughts)}\"\n",
    "        score = self.scorer.complete(score_prompt)\n",
    "        chosen_idx = 1 if \"Thought 1\" in score else 2\n",
    "        final_prompt = f\"[MODE:TOT][STEP:FINAL][CHOICE:{chosen_idx}][TICKET:{ticket.id}] {thoughts[chosen_idx-1]}\"\n",
    "        final = self.finalizer.complete(final_prompt)\n",
    "\n",
    "        return {\n",
    "            \"thoughts\": thoughts,\n",
    "            \"score\": score,\n",
    "            \"final\": final,\n",
    "        }\n",
    "\n",
    "\n",
    "tot_agent = TreeOfThoughtAgent(tot_planner, tot_scorer, tot_finalizer)\n",
    "\n",
    "\n",
    "def run_tot() -> Tuple[Dict[int, str], Dict[int, Dict[str, str]]]:\n",
    "    responses = {}\n",
    "    traces = {}\n",
    "    for ticket in TICKETS:\n",
    "        result = tot_agent.respond(ticket)\n",
    "        responses[ticket.id] = result[\"final\"]\n",
    "        traces[ticket.id] = result\n",
    "    return responses, traces\n",
    "\n",
    "\n",
    "tot_responses, tot_traces = run_tot()\n",
    "tot_scores = score_responses(tot_responses)\n",
    "tot_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f46c0",
   "metadata": {},
   "source": [
    "ToT surfaces intermediate reasoning, helping humans audit why a decision was made. Expanding more than two branches increases coverage further but at higher cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b09c02",
   "metadata": {},
   "source": [
    "## Automated prompt editing in code\n",
    "\n",
    "Finally we implement a miniature prompt editor. Starting from a weak template, we iteratively append instructions for the most frequently missed key point until coverage converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fb3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_templates = {\n",
    "    0: \"You are OrbitServe support. Reply briefly and stay polite.\",\n",
    "}\n",
    "\n",
    "prompt_editor_model = FakeLLM(\n",
    "    name=\"PromptEditorLLM\",\n",
    "    scripts={\n",
    "        \"[MODE:PROMPT_EDIT][VERSION:0][TICKET:1]\": \"Hi HelioMart, engineers are working and will update soon.\",\n",
    "        \"[MODE:PROMPT_EDIT][VERSION:0][TICKET:2]\": \"NimbusBank, the export should be okay. Reach out if needed.\",\n",
    "        \"[MODE:PROMPT_EDIT][VERSION:0][TICKET:3]\": \"Wayfinder, retries paused; try later.\",\n",
    "        \"[MODE:PROMPT_EDIT][VERSION:0][TICKET:4]\": \"Aurora Health, security matters to us.\",\n",
    "        \"[MODE:PROMPT_EDIT][VERSION:1][TICKET:1]\": (\n",
    "            \"HelioMart, apologize for the disruption, confirm engineers are restoring analytics with eta 2 hours, offer csv export workaround.\"\n",
    "        ),\n",
    "        \"[MODE:PROMPT_EDIT][VERSION:1][TICKET:2]\": (\n",
    "            \"NimbusBank, reference SOC 2 signed export with signature hash, outline secure download steps, invite follow-up.\"\n",
    "        ),\n",
    "        \"[MODE:PROMPT_EDIT][VERSION:1][TICKET:3]\": (\n",
    "            \"Wayfinder, acknowledge deploy regression, cite retry queue draining, advise requeue via CLI, escalate to on-call SRE.\"\n",
    "        ),\n",
    "        \"[MODE:PROMPT_EDIT][VERSION:1][TICKET:4]\": (\n",
    "            \"Aurora Health, affirm PHI encryption, describe just-in-time access controls, note audit logging retention, share HIPAA knowledge base link.\"\n",
    "        ),\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "class PromptEditingAgent:\n",
    "    def __init__(self, base_model: FakeLLM):\n",
    "        self.base_model = base_model\n",
    "        self.version = 0\n",
    "        self.template_history = {0: prompt_templates[0]}\n",
    "\n",
    "    def generate(self, ticket: Ticket) -> str:\n",
    "        prompt = (\n",
    "            f\"[MODE:PROMPT_EDIT][VERSION:{self.version}][TICKET:{ticket.id}]\"\n",
    "            f\" Template: {self.template_history[self.version]}\"\n",
    "        )\n",
    "        return self.base_model.complete(prompt)\n",
    "\n",
    "    def evaluate_and_edit(self, responses: Dict[int, str]) -> bool:\n",
    "        scores = score_responses(responses)\n",
    "        missing_counts = defaultdict(int)\n",
    "        for missing_list in scores[\"missing\"]:\n",
    "            for kp in missing_list:\n",
    "                missing_counts[kp] += 1\n",
    "        if not missing_counts:\n",
    "            return False\n",
    "        # Pick the most commonly missing key point and add it to the prompt.\n",
    "        target_point = max(missing_counts.items(), key=lambda item: item[1])[0]\n",
    "        new_version = self.version + 1\n",
    "        self.template_history[new_version] = (\n",
    "            self.template_history[self.version]\n",
    "            + f\" Always include: {target_point}.\"\n",
    "        )\n",
    "        self.version = new_version\n",
    "        return True\n",
    "\n",
    "\n",
    "def run_prompt_editing(max_iterations: int = 2) -> Tuple[Dict[int, str], Dict[int, Dict[str, str]]]:\n",
    "    agent = PromptEditingAgent(prompt_editor_model)\n",
    "    traces = {}\n",
    "    for iteration in range(max_iterations):\n",
    "        responses = {}\n",
    "        for ticket in TICKETS:\n",
    "            responses[ticket.id] = agent.generate(ticket)\n",
    "        traces[iteration] = {\n",
    "            \"version\": agent.version,\n",
    "            \"template\": agent.template_history[agent.version],\n",
    "            \"scores\": score_responses(responses),\n",
    "        }\n",
    "        improved = agent.evaluate_and_edit(responses)\n",
    "        if not improved:\n",
    "            return responses, traces\n",
    "    # Final pass with improved prompt\n",
    "    final_responses = {}\n",
    "    for ticket in TICKETS:\n",
    "        final_responses[ticket.id] = agent.generate(ticket)\n",
    "    traces[max_iterations] = {\n",
    "        \"version\": agent.version,\n",
    "        \"template\": agent.template_history[agent.version],\n",
    "        \"scores\": score_responses(final_responses),\n",
    "    }\n",
    "    return final_responses, traces\n",
    "\n",
    "\n",
    "prompt_edit_responses, prompt_edit_traces = run_prompt_editing()\n",
    "prompt_edit_scores = score_responses(prompt_edit_responses)\n",
    "prompt_edit_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7d2523",
   "metadata": {},
   "source": [
    "The prompt editor converges after one iteration: it notices missing requirements, adds them to the template, and reaches full coverage. Unlike self-critique, this method updates the *instruction* instead of every response, which is useful for batch quality improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f1aad8",
   "metadata": {},
   "source": [
    "## Comparing all methods\n",
    "\n",
    "Let's aggregate coverage, average length, and iterations for each strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459df17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(scores: pd.DataFrame, method: str) -> Dict[str, float]:\n",
    "    return {\n",
    "        \"method\": method,\n",
    "        \"avg_coverage\": scores[\"coverage\"].mean(),\n",
    "        \"avg_length\": scores[\"length\"].mean(),\n",
    "    }\n",
    "\n",
    "comparison = pd.DataFrame([\n",
    "    summarize(baseline_summary[baseline_summary.method == \"Zero-shot\"], \"Zero-shot\"),\n",
    "    summarize(baseline_summary[baseline_summary.method == \"Few-shot\"], \"Few-shot\"),\n",
    "    summarize(self_critique_scores, \"Self-critique\"),\n",
    "    summarize(debate_scores, \"Debate\"),\n",
    "    summarize(tot_scores, \"Tree-of-thought\"),\n",
    "    summarize(prompt_edit_scores, \"Prompt editing\"),\n",
    "])\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1044bb1",
   "metadata": {},
   "source": [
    "| Method | Avg. coverage | Avg. length (words) | Iterations | When to use |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| Zero-shot | Poor (0.19) | Short (11) | 1 | Quick prototypes; manual QA. |\n",
    "| Few-shot | Moderate (0.56) | Medium (32) | 1 | Format training; low-stakes tasks. |\n",
    "| Self-critique | Excellent (1.00) | Medium (53) | 3 model calls | Balanced default when budgets allow. |\n",
    "| Debate | Excellent (1.00) | Medium (50) | 3 model calls + judge | Use when disagreements are likely or safety is critical. |\n",
    "| Tree-of-thought | Excellent (1.00) | Medium (49) | Planner + scorer + finalizer | Great for planning or multi-step reasoning. |\n",
    "| Prompt editing | Excellent (1.00) | Medium (48) | Offline iterations | Ideal for batch fixes or when runtime cost must stay low. |\n",
    "\n",
    "The numeric values match our simulation. In practice you would recompute coverage using real QA metrics or human review. Choose the simplest method that meets your quality bar, then layer more reflection only when the baseline fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1063a6",
   "metadata": {},
   "source": [
    "## Integrating external feedback\n",
    "\n",
    "To extend these loops with external signals:\n",
    "\n",
    "1. Replace `coverage_score` with a rubric grader or policy classifier. Feed the classifier output into the critique prompt.\n",
    "2. For Retrieval-Augmented Reflection, add a retrieval step that injects documentation snippets into the critique or debate prompts.\n",
    "3. Incorporate human feedback by logging critiques (from QA teams or customers) and replaying them through the prompt editor to update instructions.\n",
    "4. Track telemetry (deflection rate, CSAT) per method and down-rank methods that regress.\n",
    "\n",
    "These hooks close the gap between offline simulations and production-grade agents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614de6c",
   "metadata": {},
   "source": [
    "## Key takeaways\n",
    "\n",
    "- Reflection can be internal (self-critique, ToT, prompt editing) or social (debate, external judges). Mix and match based on failure modes.\n",
    "- Start with simple baselines; add reflection when policies or recall targets are unmet.\n",
    "- Instrument every loop with metrics (coverage, citations, hallucination rate) and cap iterations to control cost.\n",
    "- Prompt editing upgrades instructions globally, while critique/debate improve individual conversations.\n",
    "- Ground critiques with external data for sensitive domains like finance or healthcare."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-your-own-super-agents (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
